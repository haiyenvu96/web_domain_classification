{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French web domain classification\n",
    "## Graph and Text combination\n",
    "**Team: Samuel - TuAnh - HaiYen**\n",
    "\n",
    "**Note1: This notebook requires the file text_all_scores (as well as some other files such as X_train/test/other_text, other_hosts), which can be obtained by running the notebook TEXT_FINAL.ipynb**\n",
    "\n",
    "**Note2: As we have cleaned and rerun the file, the obtained results may not be as well as the final result on Kaggle.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "from os import path\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"D:\\\\Tu Beo\\\\Education\\\\AlteGrad19\\\\Data Challenge\\\\data\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and test hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "with open(path_data+\"train.csv\", 'r') as f:\n",
    "    train_data = f.read().splitlines()\n",
    "    \n",
    "train_hosts = list()\n",
    "y_train = list()\n",
    "for row in train_data:\n",
    "    host, label = row.split(\",\")\n",
    "    train_hosts.append(host)\n",
    "    y_train.append(label.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data\n",
    "with open(path_data+\"test.csv\", 'r') as f:\n",
    "    test_hosts = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56270 lines and found 28003 hosts.\n"
     ]
    }
   ],
   "source": [
    "logits_dict = {}\n",
    "with open(path_data+'text_all_scores.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and len(row) > 0:\n",
    "            logits_dict[row[0]] = row[1:]\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines and found {len(logits_dict)} hosts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  28002\n",
      "Number of edges:  319498\n"
     ]
    }
   ],
   "source": [
    "# Create a directed, weighted graph\n",
    "G = nx.read_weighted_edgelist(path_data+'edgelist.txt', create_using=nx.DiGraph())\n",
    "\n",
    "print(\"Number of nodes: \", G.number_of_nodes())\n",
    "print(\"Number of edges: \", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the average scores of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_logits(hosts, G=G, num_classes=8, logits_dict=logits_dict):\n",
    "    succ_logits = []\n",
    "    pred_logits = []\n",
    "    for host in hosts:\n",
    "        sum_logits = np.zeros(num_classes)\n",
    "        for node in G.successors(host):\n",
    "            sum_logits += np.array(logits_dict[node], dtype='float64')*G.edges[host,node]['weight']\n",
    "        if np.sum(sum_logits) == 0:\n",
    "            sum_logits = np.ones(num_classes)/num_classes\n",
    "        else:\n",
    "            sum_logits = sum_logits/np.sum(sum_logits)\n",
    "        succ_logits.append(sum_logits)\n",
    "\n",
    "        sum_logits = np.zeros(num_classes)\n",
    "        for node in G.predecessors(host):\n",
    "            sum_logits += np.array(logits_dict[node], dtype='float64')*G.edges[node, host]['weight']\n",
    "        if np.sum(sum_logits) == 0:\n",
    "            sum_logits = np.ones(num_classes)/num_classes\n",
    "        else:\n",
    "            sum_logits = sum_logits/np.sum(sum_logits)\n",
    "        pred_logits.append(sum_logits)\n",
    "    return succ_logits, pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_succ_logits, train_pred_logits = get_avg_logits(train_hosts)\n",
    "X_train_logits = np.concatenate((train_succ_logits, train_pred_logits), axis=1)\n",
    "X_train_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the tf-idf vectors of the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Uncomment and run the following block only if you did not save the matrices in the TEXT part, otherwise load the saved matrix in the block after**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Load and preprocess text\n",
    "# text = dict()\n",
    "# filenames = os.listdir(path_data+'text/text')\n",
    "# for filename in filenames:\n",
    "#     with codecs.open(path.join(path_data+'text/text/', filename), encoding='latin-1') as f: \n",
    "#         text[filename] = f.read().replace(\"\\n\", \"\").lower()\n",
    "        \n",
    "# from nltk.corpus import stopwords \n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english')).union(set(stopwords.words('french'))) \n",
    "\n",
    "# text_processed = {}\n",
    "# for j,host in enumerate(text.keys()):\n",
    "#     doc = text[host].split()\n",
    "#     for i in range(len(doc)) :\n",
    "#         doc[i] = doc[i].lstrip().rstrip()\n",
    "#         doc[i] = re.sub(r\"[\\^\\$\\-()\\\"#/@;:<>{}`+=~|\\]\\[._\\\\!?,%&*]\", \"\", doc[i])\n",
    "#         if doc[i] in stop_words or len(doc[i]) < 3:\n",
    "#             doc[i] = ''\n",
    "#     text_processed[host] = ' '.join(doc)\n",
    "\n",
    "#     if j % 500 == 0:\n",
    "#         print(str(j)+\"/\"+str(len(text.keys())), end=\"\\r\")\n",
    "        \n",
    "# # Get textual content of web hosts\n",
    "# train_text = list()\n",
    "# for host in train_hosts:\n",
    "#     if host in text_processed:\n",
    "#         train_text.append(text_processed[host])\n",
    "#     else:\n",
    "#         train_text.append('')\n",
    "# test_text = list()\n",
    "# for host in test_hosts:\n",
    "#     if host in text_processed:\n",
    "#         test_text.append(text_processed[host])\n",
    "#     else:\n",
    "#         test_text.append('')\n",
    "# other_hosts = list(set(text_processed.keys()).difference(set(train_hosts).union(test_hosts)))\n",
    "# other_data = list()\n",
    "# for host in other_hosts:\n",
    "#     if host in text_processed:\n",
    "#         other_data.append(text_processed[host])\n",
    "#     else:\n",
    "#         other_data.append('')\n",
    "        \n",
    "# # Compute tfidf vector\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', encoding='latin-1', min_df=50, max_df=2000)\n",
    "# X_train_text = vec.fit_transform(train_text)\n",
    "\n",
    "# X_test_text = vec.transform(test_text)\n",
    "# X_other_text = vec.transform(other_data)\n",
    "\n",
    "# print(\"Train matrix dimensionality: \", X_train_text.shape)\n",
    "# print(\"Test matrix dimensionality: \", X_test_text.shape)\n",
    "# print(\"Other matrix dimensionality: \", X_other_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "X_train_text = sparse.load_npz(path_data+\"X_train_text.npz\")\n",
    "X_test_text = sparse.load_npz(path_data+\"X_test_text.npz\")\n",
    "X_other_text = sparse.load_npz(path_data+\"X_other_text.npz\")\n",
    "with open(path_data+\"other_host.csv\", 'r') as f:\n",
    "    other_hosts = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hosts that are not in the graph\n",
    "other_taken_idx = []\n",
    "nodes_set = set(G.nodes())\n",
    "for i, node in enumerate(other_hosts):\n",
    "    if node in nodes_set:\n",
    "        other_taken_idx.append(i)\n",
    "X_other_text = X_other_text[other_taken_idx]\n",
    "other_hosts = np.array(other_hosts)[other_taken_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train_text_normalized = preprocessing.normalize(X_train_text, norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tsvd = TruncatedSVD(n_components=120)\n",
    "X_train_text_PCA = tsvd.fit_transform(X_train_text_normalized)\n",
    "X_train_text_PCA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125, 136)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comb = np.concatenate((X_train_text_PCA, X_train_logits), axis = 1)\n",
    "X_train_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comb_normalized = preprocessing.normalize(X_train_comb, norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN loss: 0.9039830551271831\n",
      "TRAIN accuracy: 0.6725941422594143\n",
      "VAL loss: 1.153610001311298\n",
      "VAL accuracy: 0.5633802816901409\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split\n",
    "X_train_small, X_val, y_train_small, y_val = train_test_split(X_train_comb_normalized, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "clf.fit(X_train_small, y_train_small)\n",
    "\n",
    "y_pred = clf.predict_proba(X_train_small)\n",
    "print(\"TRAIN loss:\", log_loss(y_train_small, y_pred))\n",
    "\n",
    "y_pred_label = clf.predict(X_train_small)\n",
    "print(\"TRAIN accuracy:\", accuracy_score(y_train_small, y_pred_label))\n",
    "\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "print(\"VAL loss:\", log_loss(y_val, y_pred))\n",
    "\n",
    "y_pred_label = clf.predict(X_val)\n",
    "print(\"VAL accuracy:\", accuracy_score(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output all the results (for next iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_succ_logits, other_pred_logits = get_avg_logits(other_hosts)\n",
    "test_succ_logits, test_pred_logits = get_avg_logits(test_hosts)\n",
    "X_other_logits = np.concatenate((other_succ_logits, other_pred_logits), axis=1)\n",
    "X_test_logits = np.concatenate((test_succ_logits, test_pred_logits), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text_normalized = preprocessing.normalize(X_test_text, norm='max', axis=1)\n",
    "X_other_text_normalized = preprocessing.normalize(X_other_text, norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text_PCA = tsvd.transform(X_test_text_normalized)\n",
    "X_other_text_PCA = tsvd.transform(X_other_text_normalized)\n",
    "X_test_comb_normalized =  preprocessing.normalize(np.concatenate((X_test_text_PCA, X_test_logits), axis = 1), norm='max', axis=1)\n",
    "X_other_comb_normalized =  preprocessing.normalize(np.concatenate((X_other_text_PCA, X_other_logits), axis = 1), norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN loss: 0.9116297410661994\n",
      "TRAIN accuracy: 0.6635294117647059\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "clf.fit(X_train_comb_normalized, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_train_comb_normalized)\n",
    "print(\"TRAIN loss:\", log_loss(y_train, y_pred))\n",
    "\n",
    "y_pred_label = clf.predict(X_train_comb_normalized)\n",
    "print(\"TRAIN accuracy:\", accuracy_score(y_train, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict_proba(X_train_comb_normalized)\n",
    "y_test_pred = clf.predict_proba(X_test_comb_normalized)\n",
    "y_other_pred = clf.predict_proba(X_other_comb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to the file\n",
    "with open(path_data+'textgraph_all_scores.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = clf.classes_.tolist()\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i, host in enumerate(train_hosts):\n",
    "        lst = y_train_pred[i,:].tolist()\n",
    "        lst.insert(0, host)\n",
    "        writer.writerow(lst)\n",
    "    for i, host in enumerate(test_hosts):\n",
    "        lst = y_test_pred[i,:].tolist()\n",
    "        lst.insert(0, host)\n",
    "        writer.writerow(lst)\n",
    "    for i, host in enumerate(other_hosts):\n",
    "        lst = y_other_pred[i,:].tolist()\n",
    "        lst.insert(0, host)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56268 lines and found 28002 hosts.\n"
     ]
    }
   ],
   "source": [
    "logits_dict2 = {}\n",
    "with open(path_data+'textgraph_all_scores.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count > 0 and len(row) > 0:\n",
    "            logits_dict2[row[0]] = row[1:]\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines and found {len(logits_dict2)} hosts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_succ_logits, train_pred_logits = get_avg_logits(train_hosts, logits_dict=logits_dict2)\n",
    "X_train_logits = np.concatenate((train_succ_logits, train_pred_logits), axis=1)\n",
    "X_train_comb_normalized =  preprocessing.normalize(np.concatenate((X_train_text_PCA, X_train_logits), axis = 1), norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN loss: 0.89076671160758\n",
      "TRAIN accuracy: 0.6767782426778243\n",
      "VAL loss: 1.1625661743773452\n",
      "VAL accuracy: 0.5774647887323944\n"
     ]
    }
   ],
   "source": [
    "X_train_small, X_val, y_train_small, y_val = train_test_split(X_train_comb_normalized, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Use logistic regression to classify the webpages of the test set\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "clf.fit(X_train_small, y_train_small)\n",
    "\n",
    "y_pred = clf.predict_proba(X_train_small)\n",
    "print(\"TRAIN loss:\", log_loss(y_train_small, y_pred))\n",
    "\n",
    "y_pred_label = clf.predict(X_train_small)\n",
    "print(\"TRAIN accuracy:\", accuracy_score(y_train_small, y_pred_label))\n",
    "\n",
    "y_pred = clf.predict_proba(X_val)\n",
    "print(\"VAL loss:\", log_loss(y_val, y_pred))\n",
    "\n",
    "y_pred_label = clf.predict(X_val)\n",
    "print(\"VAL accuracy:\", accuracy_score(y_val, y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set(y_train)\n",
    "num_classes = len(label_set)\n",
    "label_set_map = {'health/medical': 3,\n",
    " 'sports': 6,\n",
    " 'entertainment': 2,\n",
    " 'politics/government/law': 5,\n",
    " 'business/finance': 0,\n",
    " 'tech/science': 7,\n",
    " 'news/press': 4,\n",
    " 'education/research': 1}\n",
    "y_train_num = [label_set_map[label] for label in y_train]\n",
    "y_train_category = to_categorical(y_train_num, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               17536     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 26,312\n",
      "Trainable params: 26,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_cst = 0.00001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_comb_normalized.shape[1],kernel_regularizer=regularizers.l2(reg_cst)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(64,kernel_regularizer=regularizers.l2(reg_cst)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(32, activation='relu',kernel_regularizer=regularizers.l2(reg_cst)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l2(reg_cst)))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2018 samples, validate on 107 samples\n",
      "Epoch 1/20\n",
      "2018/2018 [==============================] - 0s 161us/step - loss: 1.8756 - acc: 0.3380 - val_loss: 1.2369 - val_acc: 0.6542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.23694, saving model to D:\\Tu Beo\\Education\\AlteGrad19\\Data Challenge\\data\\data\\weights/weights.hdf5\n",
      "Epoch 2/20\n",
      "2018/2018 [==============================] - 0s 78us/step - loss: 1.5202 - acc: 0.4757 - val_loss: 0.9706 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.23694 to 0.97057, saving model to D:\\Tu Beo\\Education\\AlteGrad19\\Data Challenge\\data\\data\\weights/weights.hdf5\n",
      "Epoch 3/20\n",
      "2018/2018 [==============================] - 0s 78us/step - loss: 1.3082 - acc: 0.5654 - val_loss: 0.9400 - val_acc: 0.7009\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.97057 to 0.94002, saving model to D:\\Tu Beo\\Education\\AlteGrad19\\Data Challenge\\data\\data\\weights/weights.hdf5\n",
      "Epoch 4/20\n",
      "2018/2018 [==============================] - 0s 74us/step - loss: 1.2192 - acc: 0.5852 - val_loss: 0.9688 - val_acc: 0.6542\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.94002\n",
      "Epoch 5/20\n",
      "2018/2018 [==============================] - 0s 91us/step - loss: 1.1489 - acc: 0.6110 - val_loss: 0.8771 - val_acc: 0.7570\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.94002 to 0.87708, saving model to D:\\Tu Beo\\Education\\AlteGrad19\\Data Challenge\\data\\data\\weights/weights.hdf5\n",
      "Epoch 6/20\n",
      "2018/2018 [==============================] - 0s 72us/step - loss: 1.0987 - acc: 0.6264 - val_loss: 0.9963 - val_acc: 0.6916\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.87708\n",
      "Epoch 7/20\n",
      "2018/2018 [==============================] - 0s 75us/step - loss: 1.0694 - acc: 0.6254 - val_loss: 1.0757 - val_acc: 0.6355\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.87708\n",
      "Epoch 8/20\n",
      "2018/2018 [==============================] - 0s 74us/step - loss: 1.0327 - acc: 0.6442 - val_loss: 0.8795 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.87708\n",
      "Epoch 9/20\n",
      "2018/2018 [==============================] - 0s 74us/step - loss: 0.9988 - acc: 0.6482 - val_loss: 1.0745 - val_acc: 0.6355\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.87708\n",
      "Epoch 10/20\n",
      "2018/2018 [==============================] - 0s 74us/step - loss: 0.9753 - acc: 0.6516 - val_loss: 1.0719 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.87708\n",
      "Epoch 11/20\n",
      "2018/2018 [==============================] - 0s 74us/step - loss: 0.9519 - acc: 0.6596 - val_loss: 1.0177 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.87708\n",
      "Epoch 12/20\n",
      "2018/2018 [==============================] - 0s 67us/step - loss: 0.9215 - acc: 0.6705 - val_loss: 1.0901 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.87708\n",
      "Epoch 13/20\n",
      "2018/2018 [==============================] - 0s 65us/step - loss: 0.9060 - acc: 0.6829 - val_loss: 0.9830 - val_acc: 0.6355\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.87708\n",
      "Epoch 14/20\n",
      "2018/2018 [==============================] - 0s 67us/step - loss: 0.8847 - acc: 0.6868 - val_loss: 1.0533 - val_acc: 0.5981\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.87708\n",
      "Epoch 15/20\n",
      "2018/2018 [==============================] - 0s 69us/step - loss: 0.8629 - acc: 0.6997 - val_loss: 1.0807 - val_acc: 0.5888\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.87708\n",
      "Epoch 16/20\n",
      "2018/2018 [==============================] - 0s 69us/step - loss: 0.8384 - acc: 0.7076 - val_loss: 1.0769 - val_acc: 0.5981\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.87708\n",
      "Epoch 17/20\n",
      "2018/2018 [==============================] - 0s 69us/step - loss: 0.8398 - acc: 0.6898 - val_loss: 0.9427 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.87708\n",
      "Epoch 18/20\n",
      "2018/2018 [==============================] - 0s 73us/step - loss: 0.8034 - acc: 0.7175 - val_loss: 1.0287 - val_acc: 0.6075\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.87708\n",
      "Epoch 19/20\n",
      "2018/2018 [==============================] - 0s 66us/step - loss: 0.7796 - acc: 0.7245 - val_loss: 0.9828 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.87708\n",
      "Epoch 20/20\n",
      "2018/2018 [==============================] - 0s 71us/step - loss: 0.7594 - acc: 0.7329 - val_loss: 1.2108 - val_acc: 0.5421\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.87708\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=path_data+\"weights/weights.hdf5\", verbose=1, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train_comb_normalized, y_train_category, \n",
    "          batch_size=32, epochs=20, \n",
    "          validation_split=0.05, shuffle=True, \n",
    "          callbacks = [checkpointer],\n",
    "          verbose = 1)\n",
    "\n",
    "model.load_weights(path_data+\"weights/weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL loss: 1.0313645015894808\n",
      "VAL accuracy: 0.6197183098591549\n"
     ]
    }
   ],
   "source": [
    "y_val_num = [label_set_map[label] for label in y_val]\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"VAL loss:\", log_loss(y_val_num, y_pred))\n",
    "y_pred = model.predict_classes(X_val)\n",
    "print(\"VAL accuracy:\", accuracy_score(y_val_num, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_succ_logits, test_pred_logits = get_avg_logits(test_hosts, logits_dict=logits_dict2)\n",
    "X_test_logits = np.concatenate((test_succ_logits, test_pred_logits), axis=1)\n",
    "X_test_text_PCA = tsvd.transform(X_test_text)\n",
    "X_test_comb_normalized =  preprocessing.normalize(np.concatenate((X_test_text_PCA, X_test_logits), axis = 1), norm='max', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN loss: 1.0077274528402178\n",
      "TRAIN accuracy: 0.6465882352941177\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train_comb_normalized)\n",
    "print(\"TRAIN loss:\", log_loss(y_train_num, y_pred))\n",
    "\n",
    "y_pred_label = model.predict_classes(X_train_comb_normalized)\n",
    "print(\"TRAIN accuracy:\", accuracy_score(y_train_num, y_pred_label))\n",
    "\n",
    "y_pred_nn = model.predict(X_test_comb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "with open(path_data+'textgraph_test_NN_to_kaggle.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = clf.classes_.tolist()\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_hosts):\n",
    "        lst = y_pred_nn[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN loss: 0.9011271923744552\n",
      "TRAIN accuracy: 0.6696470588235294\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "clf.fit(X_train_comb_normalized, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_train_comb_normalized)\n",
    "print(\"TRAIN loss:\", log_loss(y_train, y_pred))\n",
    "\n",
    "y_pred_label = clf.predict(X_train_comb_normalized)\n",
    "print(\"TRAIN accuracy:\", accuracy_score(y_train, y_pred_label))\n",
    "\n",
    "y_pred = clf.predict_proba(X_test_comb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "with open(path_data+'textgraph_test_logreg_to_kaggle.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = clf.classes_.tolist()\n",
    "    lst.insert(0, \"Host\")\n",
    "    writer.writerow(lst)\n",
    "    for i,test_host in enumerate(test_hosts):\n",
    "        lst = y_pred[i,:].tolist()\n",
    "        lst.insert(0, test_host)\n",
    "        writer.writerow(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
